<!doctype html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications</title>
  <meta name="description" content="Peer-reviewed papers and preprints.">
  <link rel="stylesheet" href="/assets/main.css">

  <script>
    // Dark by default; respect user choice if light
    const theme = localStorage.getItem('theme');
    if (theme === 'light') document.documentElement.classList.remove('dark');
    else document.documentElement.classList.add('dark');
  </script>

  <style>
    /* === HERO ANIMATIONS === */
    .hero-enter { opacity: 0; transform: translateY(24px); transition: opacity .8s ease-out, transform .8s ease-out; }
    .hero-enter.is-visible { opacity: 1; transform: none; }
    .hero-parallax { will-change: transform; transition: transform 0.1s linear; }
    .hero-bg-pulse { animation: bgPulse 6s ease-in-out infinite alternate; }
    @keyframes bgPulse { from { opacity:.35; transform:scale(1.1);} to { opacity:.45; transform:scale(1.2);} }

    /* Page fade */
    .fade-root { opacity: 0; transition: opacity .45s ease; }
    .fade-root.is-ready { opacity: 1; }

    /* Scroll reveal */
    .reveal { opacity: 0; transform: translateY(12px); transition: opacity .6s ease, transform .6s ease; }
    .reveal.is-visible { opacity: 1; transform: none; }

    /* Hero subtle glow */
    .glow { box-shadow: 0 0 80px rgba(56,189,248,.25), 0 0 160px rgba(56,189,248,.18) inset; }
  </style>

  <script>
    // Fade-in on load & scroll reveal
    document.addEventListener('DOMContentLoaded', () => {
      document.body.classList.add('fade-root','is-ready');
      const io = new IntersectionObserver((entries)=>entries.forEach(e=>{
        if(e.isIntersecting) e.target.classList.add('is-visible');
      }), {threshold:.12});
      document.querySelectorAll('.reveal').forEach(el=>io.observe(el));
    });

    // Fade-out on internal link nav
    document.addEventListener('click', (e)=>{
      const a = e.target.closest('a[href]');
      if(!a) return;
      const url = new URL(a.href, location.href);
      if(url.origin===location.origin && !a.hasAttribute('target') && !a.href.includes('#')) {
        e.preventDefault();
        document.body.classList.remove('is-ready');
        setTimeout(()=>{ location.href=a.href; }, 200);
      }
    });
  </script>
</head>
<script>
  // Reveal hero elements on load (so foreground fades in)
  document.addEventListener('DOMContentLoaded', () => {
    requestAnimationFrame(() => {
      document.querySelectorAll('.hero-enter').forEach(el => el.classList.add('is-visible'));
    });
  });
</script>
<body class="bg-white text-slate-900 dark:bg-slate-900 dark:text-slate-100">
  <div class="max-w-6xl mx-auto px-4">
    <header class="py-6 flex justify-between items-center">
  <a href="/" class="font-semibold text-xl">Jimmy Weaver - Head of Data Delivery</a>
  <nav class="flex items-center gap-6">
    <a href="/projects/" class="hover:underline">Projects</a>
    <a href="/publications/" class="hover:underline">Publications</a>
    <a href="/articles/" class="hover:underline">Articles</a>
    <a href="/about/" class="hover:underline">About / Resume</a>
    <button id="themeToggle" class="rounded px-3 py-1 border text-sm">Theme</button>
  </nav>
  <script>
    const btn = document.getElementById('themeToggle');
    btn?.addEventListener('click', () => {
      const root = document.documentElement;
      const dark = root.classList.toggle('dark');
      localStorage.setItem('theme', dark ? 'dark' : 'light');
    });
  </script>
</header>

    <main class="py-10"><h1 class="text-3xl font-semibold mb-6">Publications</h1>



  <div class="space-y-4">
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2014</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1016/j.neulet.2014.05.003" target="_blank" rel="noopener">Neural correlates of emotional responses to music: An EEG study</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Neuroscience Letters</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, A.Malik, F.Hwang, E.Roesch, J.Weaver, A.Kirke, D.Williams, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">This paper presents an EEG study into the neural correlates of music-induced emotions. We presented participants with a large dataset containing musical pieces in different styles, and asked them to report on their induced emotional responses. We found neural correlates of music-induced emotion in a number of frequencies over the pre-frontal cortex. Additionally, we found a set of patterns of functional connectivity, defined by inter-channel coherence measures, to be significantly different between groups of music-induced emotional responses.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1016/j.neulet.2014.05.003" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Coherence Analysis</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Imaginary Coherency</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music Tempo</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain Connectivity Analysis</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2014</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://speech.di.uoa.gr/ICMC-SMC-2014/images/VOL_2/0905.pdf" target="_blank" rel="noopener">Evaluating perceptual separation in a pilot system for affective composition</a>
          
        </h2>
        <span class="text-sm text-slate-400">· ICMC-SMC</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, E.Roesch, J.Weaver, S.Nasuto, D.Williams, A.Kirke, E.Miranda
        </p>
      

      
        <p class="mt-3 text-slate-300">Research evaluating perceptual responses to music has identified many structural features as correlates that might be incorporated in computer music systems for affectively charged algorithmic composition and/or ex- pressive music performance. In order to investigate the possible integration of isolated musical features to such a system, a discrete feature known to correlate some with emotional responses – rhythmic density – was selected from a literature review and incorporated into a prototype system. This system produces variation in rhythm density via a transformative process. A stimulus set created using this system was then subjected to a perceptual evaluation. Pairwise comparisons were used to scale differences be- tween 48 stimuli. Listener responses were analysed with Multidimensional scaling (MDS). The 2-Dimensional solution was then rotated to place the stimuli with the largest range of variation across the horizontal plane. Stimuli with variation in rhythmic density were placed further from the source material than stimuli that were generated by random permutation. This, combined with the striking similarity between the MDS scaling and that of the 2-dimensional emotional model used by some af- fective algorithmic composition systems, suggests that isolated musical feature manipulation can now be used to parametrically control affectively charged automated composition in a larger system.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://speech.di.uoa.gr/ICMC-SMC-2014/images/VOL_2/0905.pdf" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Correlation</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2014</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/EMBC.2014.6944647" target="_blank" rel="noopener">Changes in music tempo entrain movement related brain activity</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, J.Hallowell, F.Hwang, A.Kirke, A.Malik, E.Roesch, J.Weaver, D.Williams, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">The neural mechanisms of music listening and appreciation are not yet completely understood. Based on the apparent relationship between the beats per minute (tempo) of music and the desire to move (for example feet tapping) induced while listening to that music it is hypothesised that musical tempo may evoke movement related activity in the brain. Participants are instructed to listen, without moving, to a large range of musical pieces spanning a range of styles and tempos during an electroencephalogram (EEG) experiment. Event-related desynchronisation (ERD) in the EEG is observed to correlate significantly with the variance of the tempo of the musical stimuli. This suggests that the dynamics of the beat of the music may induce movement related brain activity in the motor cortex. Furthermore, significant correlations are observed between EEG activity in the alpha band over the motor cortex and the bandpower of the music in the same frequency band over time. This relationship is observed to correlate with the strength of the ERD, suggesting entrainment of motor cortical activity relates to increased ERD strength.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/EMBC.2014.6944647" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Correlation</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2014</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1080/2326263X.2014.979728" target="_blank" rel="noopener">Investigating music tempo as a feedback mechanism for closed-loop BCI control</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Brain-Computer Interfaces</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, F.Hwang, A.Kirke, A.Malik, E.Roesch, J.Weaver, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">The feedback mechanism used in a brain-computer interface (BCI) forms an integral part of the closed-loop learning process required for successful operation of a BCI. However, ultimate success of the BCI may be dependent upon the modality of the feedback used. This study explores the use of music tempo as a feedback mechanism in BCI and compares it to the more commonly used visual feedback mechanism. Three different feedback modalities are compared for a kinaesthetic motor imagery BCI: visual, auditory via music tempo, and a combined visual and auditory feedback modality. Visual feedback is provided via the position, on the y-axis, of a moving ball. In the music feedback condition, the tempo of a piece of continuously generated music is dynamically adjusted via a novel music-generation method. All the feedback mechanisms allowed users to learn to control the BCI. However, users were not able to maintain as stable control with the music tempo feedback condition as they could in the visual feedback and combined conditions. Additionally, the combined condition exhibited significantly less inter-user variability, suggesting that multi-modal feedback may lead to more robust results. Finally, common spatial patterns are used to identify participant-specific spatial filters for each of the feedback modalities. The mean optimal spatial filter obtained for the music feedback condition is observed to be more diffuse and weaker than the mean spatial filters obtained for the visual and combined feedback conditions.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1080/2326263X.2014.979728" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music Feedback</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Visual Feedback</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music Tempo</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Motor Imagery</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2015</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/CEEC.2015.7332705" target="_blank" rel="noopener">Investigating Perceived Emotional Correlates of Rhythmic Density in Algorithmic Music Composition</a>
          
        </h2>
        <span class="text-sm text-slate-400">· ACM Transactions on Applied Perception</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          D.Williams, A.Kirke, E.Miranda, I.Daly, J.Hallowell, J.Weaver, A.Malik, E.Roesch, F.Hwang, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">Affective algorithmic composition is a growing field that combines perceptually motivated affective computing strategies with novel music generation. This article presents work toward the development of one application. The long-term goal is to develop a responsive and adaptive system for inducing affect that is both controlled and validated by biophysical measures. Literature documenting perceptual responses to music identifies a variety of musical features and possible affective correlations, but perceptual evaluations of these musical features for the purposes of inclusion in a music generation system are not readily available. A discrete feature, rhythmic density (a function of note duration in each musical bar, regardless of tempo), was selected because it was shown to be well-correlated with affective responses in existing literature. A prototype system was then designed to produce controlled degrees of variation in rhythmic density via a transformative algorithm. A two-stage perceptual evaluation of a stimulus set created by this prototype was then undertaken. First, listener responses from a pairwise scaling experiment were analyzed via Multidimensional Scaling Analysis (MDS). The statistical best-fit solution was rotated such that stimuli with the largest range of variation were placed across the horizontal plane in two dimensions. In this orientation, stimuli with deliberate variation in rhythmic density appeared farther from the source material used to generate them than from stimuli generated by random permutation. Second, the same stimulus set was then evaluated according to the order suggested in the rotated two-dimensional solution in a verbal elicitation experiment. A Verbal Protocol Analysis (VPA) found that listener perception of the stimulus set varied in at least two commonly understood emotional descriptors, which might be considered affective correlates of rhythmic density. Thus, these results further corroborate previous studies wherein musical parameters are monitored for changes in emotional expression and that some similarly parameterized control of perceived emotional content in an affective algorithmic composition system can be achieved and provide a methodology for evaluating and including further possible musical features in such a system. Some suggestions regarding the test procedure and analysis techniques are also documented here.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/CEEC.2015.7332705" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Applied Computing</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Sound and Music Computing</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2015</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/CEEC.2015.7332705" target="_blank" rel="noopener">Towards human-computer music interaction: Evaluation of an affectively-driven music generator via galvanic skin response measures</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, A.Malik, J.Weaver, F.Hwang, S.Nasuto, D.Williams
        </p>
      

      
        <p class="mt-3 text-slate-300">An affectively driven music generation system is described and evaluated. The system is developed for the intended eventual use in human-computer interaction systems such as brain-computer music interfaces. It is evaluated for its ability to induce changes in a listeners affective state. The affectively-driven algorithmic composition system was used to generate a stimulus set covering 9 discrete sectors of a 2-dimensional affective space by means of a 16 channel feedforward artificial neural network. This system was used to generate 90 short pieces of music with specific affective intentions, 10 stimuli for each of the 9 sectors in the affective space. These pieces were played to 20 healthy participants, and it was observed that the music generation system induced the intended affective states in the participants. This is further verified by inspecting the galvanic skin response recorded from participants.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/CEEC.2015.7332705" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Galvanic Skin Response Measurements</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Artificial Neural Network</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">General System</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Affective States</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Feed-forward Artificial Neural Network</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Time Series</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2015</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1016/j.bandc.2015.08.003" target="_blank" rel="noopener">Music-induced emotions can be predicted from a combination of brain activity and acoustic features</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Brain and Cognition</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, J.Hallowell, F.Hwang, A.Kirke, A.Malik, J.Weaver, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">It is widely acknowledged that music can communicate and induce a wide range of emotions in the listener. However, music is a highly-complex audio signal composed of a wide range of complex time- and frequency-varying components. Additionally, music-induced emotions are known to differ greatly between listeners. Therefore, it is not immediately clear what emotions will be induced in a given individual by a piece of music. We attempt to predict the music-induced emotional response in a listener by measuring the activity in the listeners electroencephalogram (EEG). We combine these measures with acoustic descriptors of the music, an approach that allows us to consider music as a complex set of time-varying acoustic features, independently of any specific music theory. Regression models are found which allow us to predict the music-induced emotions of our participants with a correlation between the actual and predicted responses of up to. This regression fit suggests that over 20% of the variance of the participant’s music induced emotions can be predicted by their neural activity and the properties of the music. Given the large amount of noise, non-stationarity, and non-linearity in both EEG and music, this is an encouraging result. Additionally, the combination of measures of brain activity and acoustic features describing the music played to our participants allows us to predict music-induced emotions with significantly higher accuracies than either feature type alone ().</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1016/j.bandc.2015.08.003" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">EEG</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">GSR</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Affective State Detection</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">pPrsonalised Affective State Detection</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2015</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/ACII.2015.7344685" target="_blank" rel="noopener">Identifying music-induced emotions from EEG for use in brain-computer music interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, A.Malik, J.Weaver, F.Hwang, S.Nasuto, D.Williams, A.Kirke, E.Miranda
        </p>
      

      
        <p class="mt-3 text-slate-300">Brain-computer music interfaces (BCMI) provide a method to modulate an individuals affective state via the selection or generation of music according to their current affective state. Potential applications of such systems may include entertainment of therapeutic applications. We outline a proposed design for such a BCMI and seek a method for automatically differentiating different music induced affective states. Band-power features are explored for use in automatically identifying music-induced affective states. Additionally, a linear discriminant analysis classifier and a support vector machine are evaluated with respect to their ability to classify music induced affective states from the electroencephalogram recorded during a BCMI calibration task. Accuracies of up to 79.5% (p &lt;; 0.001) are achieved with the support vector machine.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/ACII.2015.7344685" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Support Vector Machines</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Calibration</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain Models</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2016</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1088/1741-2560/13/4/046022" target="_blank" rel="noopener">Affective brain–computer music interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Journal of Neural Engineering</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, A.Kirke, J.Weaver, A.Malik, F.Hwang
        </p>
      

      
        <p class="mt-3 text-slate-300">Objective. We aim to develop and evaluate an affective brain–computer music interface (aBCMI) for modulating the affective states of its users. Approach. An aBCMI is constructed to detect a user&#39;s current affective state and attempt to modulate it in order to achieve specific objectives (for example, making the user calmer or happier) by playing music which is generated according to a specific affective target by an algorithmic music composition system and a case-based reasoning system. The system is trained and tested in a longitudinal study on a population of eight healthy participants, with each participant returning for multiple sessions. Main results. The final online aBCMI is able to detect its users current affective states with classification accuracies of up to 65% (3 class, ) and modulate its user&#39;s affective states significantly above chance level . Significance. Our system represents one of the first demonstrations of an online aBCMI that is able to accurately detect and respond to user&#39;s affective states. Possible applications include use in music therapy and entertainment.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1088/1741-2560/13/4/046022" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2017</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1145/305900" target="_blank" rel="noopener">Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System</a>
          
        </h2>
        <span class="text-sm text-slate-400">· ACM Transactions on Applied Perception</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          D.Williams, A.Kirke, E.Miranda, I.Daly, F.Hwang, J.Weaver, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">Affectively driven algorithmic composition (AAC) is a rapidly growing field that exploits computer-aided composition in order to generate new music with particular emotional qualities or affective intentions. An AAC system was devised in order to generate a stimulus set covering nine discrete sectors of a two-dimensional emotion space by means of a 16-channel feed-forward artificial neural network. This system was used to generate a stimulus set of short pieces of music, which were rendered using a sampled piano timbre and evaluated by a group of experienced listeners who ascribed a two-dimensional valence-arousal coordinate to each stimulus. The underlying musical feature set, initially drawn from the literature, was subsequently adjusted by amplifying or attenuating the quantity of each feature in order to maximize the spread of stimuli in the valence-arousal space before a second listener evaluation was conducted. This process was repeated a third time in order to maximize the spread of valence-arousal coordinates ascribed to the generated stimulus set in comparison to a spread taken from an existing prerated database of stimuli, demonstrating that this prototype AAC system is capable of creating short sequences of music with a slight improvement on the range of emotion found in a stimulus set comprised of real-world, traditionally composed musical excerpts.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1145/305900" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Applied Computing</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Sound and Music Computing</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2017</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.3389/fnhum.2017.00502" target="_blank" rel="noopener">Directed Motor-Auditory EEG Connectivity Is Modulated by Music Tempo</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Frontiers in Human Neuroscience</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          N.Nicolaou, A.Malik, I.Daly, J.Weaver, F.Hwang, A.Kirke, E.Roesch, D.Williams, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">Beat perception is fundamental to how we experience music, and yet the mechanism behind this spontaneous building of the internal beat representation is largely unknown. Existing findings support links between the tempo (speed) of the beat and enhancement of electroencephalogram (EEG) activity at tempo-related frequencies, but there are no studies looking at how tempo may affect the underlying long-range interactions between EEG activity at different electrodes. The present study investigates these long-range interactions using EEG activity recorded from 21 volunteers listening to music stimuli played at 4 different tempi (50, 100, 150 and 200 beats per minute). The music stimuli consisted of piano excerpts designed to convey the emotion of “peacefulness”. Noise stimuli with an identical acoustic content to the music excerpts were also presented for comparison purposes. The brain activity interactions were characterized with the imaginary part of coherence (iCOH) in the frequency range 1.5–18 Hz (δ, θ, α and lower β) between all pairs of EEG electrodes for the four tempi and the music/noise conditions, as well as a baseline resting state (RS) condition obtained at the start of the experimental task. Our findings can be summarized as follows: (a) there was an ongoing long-range interaction in the RS engaging fronto-posterior areas; (b) this interaction was maintained in both music and noise, but its strength and directionality were modulated as a result of acoustic stimulation; (c) the topological patterns of iCOH were similar for music, noise and RS, however statistically significant differences in strength and direction of iCOH were identified; and (d) tempo had an effect on the direction and strength of motor-auditory interactions. Our findings are in line with existing literature and illustrate a part of the mechanism by which musical stimuli with different tempi can entrain changes in cortical activity.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.3389/fnhum.2017.00502" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Coherence Analysis</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Imaginary Coherency</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music Tempo</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain Connectivity Analysis</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2018</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/TAFFC.2018.2801811" target="_blank" rel="noopener">Personalised, Multi-Modal, Affective State Detection for Hybrid Brain-Computer Music Interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, A.Malik, J.Weaver, A.Kirke, F.Hwang
        </p>
      

      
        <p class="mt-3 text-slate-300">Brain-computer music interfaces (BCMIs) may be used to modulate affective states, with applications in music therapy, composition, and entertainment. However, for such systems to work they need to be able to reliably detect their user&#39;s current affective state. We present a method for personalised affective state detection for use in BCMI. We compare it to a population-based detection method trained on 17 users and demonstrate that personalised affective state detection is significantly (p &lt;; 0.01) more accurate, with average improvements in accuracy of 10.2 percent for valence and 9.3 percent for arousal. We also compare a hybrid BCMI (a BCMI that combines physiological signals with neurological signals) to a conventional BCMI design (one based upon the use of only EEG features) and demonstrate that the hybrid design results in a significant (p &lt;; 0.01) 6.2 percent improvement in performance for arousal classification and a significant (p &lt;; 0.01) 5.9 percent improvement for valence classification.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/TAFFC.2018.2801811" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">EEG</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">GSR</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Affective State Detection</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Personalised Affective State Detection</span>
          
        </div>
      
    </article>
    
  </div>

</main>
    <footer class="py-10 border-t border-slate-200 dark:border-slate-800 text-sm">
  <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-4">
    <p>© Jimmy Weaver</p>
    <div class="flex gap-4">
      <a href="https://github.com/drj1m" class="hover:underline">GitHub</a>
      <a href="https://www.linkedin.com/in/jimmy-weaver-data-scientist/" class="hover:underline">LinkedIn</a>
      <a href="https://medium.com/@jceweaver" class="hover:underline">Medium</a>
    </div>
  </div>
</footer>

  </div>

  <!-- Optional: tiny parallax helper (page-wide) -->
  <script>
    const hero = document.querySelector('.hero-parallax');
    if (hero) {
      window.addEventListener('scroll', () => {
        const offset = window.scrollY * 0.1;  // tune depth
        hero.style.transform = `translateY(${offset}px)`;
      });
    }
  </script>
</body>
</html>