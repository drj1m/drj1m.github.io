<!doctype html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications</title>
  <meta name="description" content="Peer-reviewed papers and preprints.">
  <link rel="stylesheet" href="/assets/main.css">

  <script>
    // Dark by default; respect user choice if light
    const theme = localStorage.getItem('theme');
    if (theme === 'light') document.documentElement.classList.remove('dark');
    else document.documentElement.classList.add('dark');
  </script>

  <style>
    /* === HERO ANIMATIONS === */
    .hero-enter { opacity: 0; transform: translateY(24px); transition: opacity .8s ease-out, transform .8s ease-out; }
    .hero-enter.is-visible { opacity: 1; transform: none; }
    .hero-parallax { will-change: transform; transition: transform 0.1s linear; }
    .hero-bg-pulse { animation: bgPulse 6s ease-in-out infinite alternate; }
    @keyframes bgPulse { from { opacity:.35; transform:scale(1.1);} to { opacity:.45; transform:scale(1.2);} }

    /* Page fade */
    .fade-root { opacity: 0; transition: opacity .45s ease; }
    .fade-root.is-ready { opacity: 1; }

    /* Scroll reveal */
    .reveal { opacity: 0; transform: translateY(12px); transition: opacity .6s ease, transform .6s ease; }
    .reveal.is-visible { opacity: 1; transform: none; }

    /* Hero subtle glow */
    .glow { box-shadow: 0 0 80px rgba(56,189,248,.25), 0 0 160px rgba(56,189,248,.18) inset; }
  </style>

  <script>
    // Fade-in on load & scroll reveal
    document.addEventListener('DOMContentLoaded', () => {
      document.body.classList.add('fade-root','is-ready');
      const io = new IntersectionObserver((entries)=>entries.forEach(e=>{
        if(e.isIntersecting) e.target.classList.add('is-visible');
      }), {threshold:.12});
      document.querySelectorAll('.reveal').forEach(el=>io.observe(el));
    });

    // Fade-out on internal link nav
    document.addEventListener('click', (e)=>{
      const a = e.target.closest('a[href]');
      if(!a) return;
      const url = new URL(a.href, location.href);
      if(url.origin===location.origin && !a.hasAttribute('target') && !a.href.includes('#')) {
        e.preventDefault();
        document.body.classList.remove('is-ready');
        setTimeout(()=>{ location.href=a.href; }, 200);
      }
    });
  </script>
</head>
<script>
  // Reveal hero elements on load (so foreground fades in)
  document.addEventListener('DOMContentLoaded', () => {
    requestAnimationFrame(() => {
      document.querySelectorAll('.hero-enter').forEach(el => el.classList.add('is-visible'));
    });
  });
</script>
<body class="bg-white text-slate-900 dark:bg-slate-900 dark:text-slate-100">
  <div class="max-w-6xl mx-auto px-4">
    <header class="py-6 flex justify-between items-center">
  <a href="/" class="font-semibold text-xl">Jimmy Weaver - Head of Data Delivery</a>
  <nav class="flex items-center gap-6">
    <a href="/projects/" class="hover:underline">Projects</a>
    <a href="/publications/" class="hover:underline">Publications</a>
    <a href="/articles/" class="hover:underline">Articles</a>
    <a href="/about/" class="hover:underline">About / Resume</a>
    <button id="themeToggle" class="rounded px-3 py-1 border text-sm">Theme</button>
  </nav>
  <script>
    const btn = document.getElementById('themeToggle');
    btn?.addEventListener('click', () => {
      const root = document.documentElement;
      const dark = root.classList.toggle('dark');
      localStorage.setItem('theme', dark ? 'dark' : 'light');
    });
  </script>
</header>

    <main class="py-10"><h1 class="text-3xl font-semibold mb-6">Publications</h1>



  <div class="space-y-4">
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2015</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/ACII.2015.7344685" target="_blank" rel="noopener">Identifying music-induced emotions from EEG for use in brain-computer music interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, A.Malik, J.Weaver, F.Hwang, S.Nasuto, D.Williams, A.Kirke, E.Miranda
        </p>
      

      
        <p class="mt-3 text-slate-300">Brain-computer music interfaces (BCMI) provide a method to modulate an individuals affective state via the selection or generation of music according to their current affective state. Potential applications of such systems may include entertainment of therapeutic applications. We outline a proposed design for such a BCMI and seek a method for automatically differentiating different music induced affective states. Band-power features are explored for use in automatically identifying music-induced affective states. Additionally, a linear discriminant analysis classifier and a support vector machine are evaluated with respect to their ability to classify music induced affective states from the electroencephalogram recorded during a BCMI calibration task. Accuracies of up to 79.5% (p &lt;; 0.001) are achieved with the support vector machine.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/ACII.2015.7344685" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Electroencephalography</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Music</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Support vector machines</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Calibration</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">Brain models</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2016</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1088/1741-2560/13/4/046022" target="_blank" rel="noopener">Affective brain–computer music interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Journal of Neural Engineering</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, A.Kirke, J.Weaver, A.Malik, F.Hwang
        </p>
      

      
        <p class="mt-3 text-slate-300">Objective. We aim to develop and evaluate an affective brain–computer music interface (aBCMI) for modulating the affective states of its users. Approach. An aBCMI is constructed to detect a user&#39;s current affective state and attempt to modulate it in order to achieve specific objectives (for example, making the user calmer or happier) by playing music which is generated according to a specific affective target by an algorithmic music composition system and a case-based reasoning system. The system is trained and tested in a longitudinal study on a population of eight healthy participants, with each participant returning for multiple sessions. Main results. The final online aBCMI is able to detect its users current affective states with classification accuracies of up to 65% (3 class, ) and modulate its user&#39;s affective states significantly above chance level . Significance. Our system represents one of the first demonstrations of an online aBCMI that is able to accurately detect and respond to user&#39;s affective states. Possible applications include use in music therapy and entertainment.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1088/1741-2560/13/4/046022" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">electroencephalography (EEG)</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2017</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1145/305900" target="_blank" rel="noopener">Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System</a>
          
        </h2>
        <span class="text-sm text-slate-400">· ACM Transactions on Applied Perception</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          D.Williams, A.Kirke, E.Miranda, I.Daly, F.Hwang, J.Weaver, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">Affectively driven algorithmic composition (AAC) is a rapidly growing field that exploits computer-aided composition in order to generate new music with particular emotional qualities or affective intentions. An AAC system was devised in order to generate a stimulus set covering nine discrete sectors of a two-dimensional emotion space by means of a 16-channel feed-forward artificial neural network. This system was used to generate a stimulus set of short pieces of music, which were rendered using a sampled piano timbre and evaluated by a group of experienced listeners who ascribed a two-dimensional valence-arousal coordinate to each stimulus. The underlying musical feature set, initially drawn from the literature, was subsequently adjusted by amplifying or attenuating the quantity of each feature in order to maximize the spread of stimuli in the valence-arousal space before a second listener evaluation was conducted. This process was repeated a third time in order to maximize the spread of valence-arousal coordinates ascribed to the generated stimulus set in comparison to a spread taken from an existing prerated database of stimuli, demonstrating that this prototype AAC system is capable of creating short sequences of music with a slight improvement on the range of emotion found in a stimulus set comprised of real-world, traditionally composed musical excerpts.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1145/305900" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">applied computing</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">sound and music computing</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2017</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.3389/fnhum.2017.00502" target="_blank" rel="noopener">Directed Motor-Auditory EEG Connectivity Is Modulated by Music Tempo</a>
          
        </h2>
        <span class="text-sm text-slate-400">· Frontiers in Human Neuroscience</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          N.Nicolaou, A.Malik, I.Daly, J.Weaver, F.Hwang, A.Kirke, E.Roesch, D.Williams, E.Miranda, S.Nasuto
        </p>
      

      
        <p class="mt-3 text-slate-300">Beat perception is fundamental to how we experience music, and yet the mechanism behind this spontaneous building of the internal beat representation is largely unknown. Existing findings support links between the tempo (speed) of the beat and enhancement of electroencephalogram (EEG) activity at tempo-related frequencies, but there are no studies looking at how tempo may affect the underlying long-range interactions between EEG activity at different electrodes. The present study investigates these long-range interactions using EEG activity recorded from 21 volunteers listening to music stimuli played at 4 different tempi (50, 100, 150 and 200 beats per minute). The music stimuli consisted of piano excerpts designed to convey the emotion of “peacefulness”. Noise stimuli with an identical acoustic content to the music excerpts were also presented for comparison purposes. The brain activity interactions were characterized with the imaginary part of coherence (iCOH) in the frequency range 1.5–18 Hz (δ, θ, α and lower β) between all pairs of EEG electrodes for the four tempi and the music/noise conditions, as well as a baseline resting state (RS) condition obtained at the start of the experimental task. Our findings can be summarized as follows: (a) there was an ongoing long-range interaction in the RS engaging fronto-posterior areas; (b) this interaction was maintained in both music and noise, but its strength and directionality were modulated as a result of acoustic stimulation; (c) the topological patterns of iCOH were similar for music, noise and RS, however statistically significant differences in strength and direction of iCOH were identified; and (d) tempo had an effect on the direction and strength of motor-auditory interactions. Our findings are in line with existing literature and illustrate a part of the mechanism by which musical stimuli with different tempi can entrain changes in cortical activity.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.3389/fnhum.2017.00502" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">coherence analysis</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">imaginary coherency</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">electroencephalography (EEG)</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">music tempo</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">brain connectivity analysis</span>
          
        </div>
      
    </article>
    
    <article class="p-5 rounded-2xl border border-slate-800">
      <div class="flex flex-wrap items-baseline gap-2">
        <span class="text-sm text-slate-400">2018</span>
        <h2 class="font-semibold">
          
            <a class="hover:underline" href="https://doi.org/10.1109/TAFFC.2018.2801811" target="_blank" rel="noopener">Personalised, Multi-Modal, Affective State Detection for Hybrid Brain-Computer Music Interfacing</a>
          
        </h2>
        <span class="text-sm text-slate-400">· IEEE</span>
      </div>

      
        <p class="text-sm text-slate-300 mt-1">
          I.Daly, D.Williams, A.Malik, J.Weaver, A.Kirke, F.Hwang
        </p>
      

      
        <p class="mt-3 text-slate-300">Brain-computer music interfaces (BCMIs) may be used to modulate affective states, with applications in music therapy, composition, and entertainment. However, for such systems to work they need to be able to reliably detect their user&#39;s current affective state. We present a method for personalised affective state detection for use in BCMI. We compare it to a population-based detection method trained on 17 users and demonstrate that personalised affective state detection is significantly (p &lt;; 0.01) more accurate, with average improvements in accuracy of 10.2 percent for valence and 9.3 percent for arousal. We also compare a hybrid BCMI (a BCMI that combines physiological signals with neurological signals) to a conventional BCMI design (one based upon the use of only EEG features) and demonstrate that the hybrid design results in a significant (p &lt;; 0.01) 6.2 percent improvement in performance for arousal classification and a significant (p &lt;; 0.01) 5.9 percent improvement for valence classification.</p>
      

      <div class="mt-3 flex flex-wrap gap-2">
        <a class="px-2 py-1 text-xs rounded-full border border-slate-700" href="https://doi.org/10.1109/TAFFC.2018.2801811" target="_blank" rel="noopener">DOI</a>
        
        
        
        
        
      </div>

      
        <div class="mt-3 flex flex-wrap gap-2">
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">EEG</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">GSR</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">affective state detection</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">BCMI</span>
          
            <span class="px-2 py-1 rounded-full text-xs border border-slate-700">personalised affective state detection</span>
          
        </div>
      
    </article>
    
  </div>

</main>
    <footer class="py-10 border-t border-slate-200 dark:border-slate-800 text-sm">
  <div class="flex flex-col sm:flex-row justify-between items-start sm:items-center gap-4">
    <p>© Jimmy Weaver</p>
    <div class="flex gap-4">
      <a href="https://github.com/drj1m" class="hover:underline">GitHub</a>
      <a href="https://www.linkedin.com/in/jimmy-weaver-data-scientist/" class="hover:underline">LinkedIn</a>
      <a href="https://medium.com/@jceweaver" class="hover:underline">Medium</a>
    </div>
  </div>
</footer>

  </div>

  <!-- Optional: tiny parallax helper (page-wide) -->
  <script>
    const hero = document.querySelector('.hero-parallax');
    if (hero) {
      window.addEventListener('scroll', () => {
        const offset = window.scrollY * 0.1;  // tune depth
        hero.style.transform = `translateY(${offset}px)`;
      });
    }
  </script>
</body>
</html>